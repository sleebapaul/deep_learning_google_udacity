{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/Users/jenkins/sleeba/deep_learning_udacity/not_mnist/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying L2 regularization in Logistic Regression using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot encoded labels are not supported by `sklearn` Logistic Regression model. So the data needs to be modelled as single column labels spanning from 0-9. Train dataset is needed to flattened.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "def reformat(dataset):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    return dataset\n",
    "train_dataset_log = reformat(train_dataset)\n",
    "train_labels_log = train_labels\n",
    "valid_dataset_log = reformat(valid_dataset)\n",
    "valid_labels_log = valid_labels\n",
    "test_dataset_log = reformat(test_dataset)\n",
    "test_labels_log  = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimension:  (200000, 784)\n",
      "Training data labels dimension:  (200000,)\n",
      "Validation data dimension:  (10000, 784)\n",
      "Validation data labels dimension:  (10000,)\n",
      "Testing data dimension:  (10000, 784)\n",
      "Testing data labels dimension:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data dimension: \",train_dataset_log.shape)\n",
    "print(\"Training data labels dimension: \", train_labels_log.shape)\n",
    "print(\"Validation data dimension: \",valid_dataset_log.shape)\n",
    "print(\"Validation data labels dimension: \", valid_labels_log.shape)\n",
    "print(\"Testing data dimension: \",test_dataset_log.shape)\n",
    "print(\"Testing data labels dimension: \", test_labels_log.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly selecting examples to train, validate and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_train = np.random.choice(np.arange(len(train_dataset_log)), 60000, replace=False)\n",
    "idx_valid = np.random.choice(np.arange(len(valid_dataset_log)), 10000, replace=False)\n",
    "idx_test = np.random.choice(np.arange(len(test_dataset_log)), 10000, replace=False)\n",
    "train_sample_log = train_dataset_log[idx_train]\n",
    "train_label_sample_log = train_labels_log[idx_train]\n",
    "valid_sample_log = valid_dataset_log[idx_valid]\n",
    "valid_label_sample_log = valid_labels_log[idx_valid]\n",
    "test_sample_log = test_dataset_log[idx_test]\n",
    "test_label_sample_log = test_labels_log[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sample dimension:  (60000, 784)\n",
      "Training data labels sample dimension:  (60000,)\n",
      "Validation data sample dimension:  (10000, 784)\n",
      "Validation data labels sample dimension:  (10000,)\n",
      "Testing data sample dimension:  (10000, 784)\n",
      "Testing data labels sample dimension:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data sample dimension: \",train_sample_log.shape)\n",
    "print(\"Training data labels sample dimension: \", train_label_sample_log.shape)\n",
    "print(\"Validation data sample dimension: \",valid_sample_log.shape)\n",
    "print(\"Validation data labels sample dimension: \", valid_label_sample_log.shape)\n",
    "print(\"Testing data sample dimension: \",test_sample_log.shape)\n",
    "print(\"Testing data labels sample dimension: \", test_label_sample_log.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and checking accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "\n",
      "Regularization rate:  1.0\n",
      "Prediction accuracy in train set :  0.903\n",
      "Prediction accuracy in validation set :  0.796\n",
      "Prediction accuracy in test set :  0.872666666667\n",
      "-------------------------------------------\n",
      "[LibLinear]\n",
      "\n",
      "Regularization rate:  0.01\n",
      "Prediction accuracy in train set :  0.8367\n",
      "Prediction accuracy in validation set :  0.808333333333\n",
      "Prediction accuracy in test set :  0.889\n",
      "-------------------------------------------\n",
      "[LibLinear]\n",
      "\n",
      "Regularization rate:  0.001\n",
      "Prediction accuracy in train set :  0.8142\n",
      "Prediction accuracy in validation set :  0.800333333333\n",
      "Prediction accuracy in test set :  0.870666666667\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "regularization_rate = [1.0, 0.01, 0.001] # this is same as beta = [1, 0.01, 0.001]\n",
    "for i in regularization_rate:\n",
    "    model = LogisticRegression(verbose=1, n_jobs=-1, max_iter=800, penalty=\"l2\", C=i)\n",
    "    model.fit(train_sample_log, train_label_sample_log)\n",
    "    predicted_train_log = model.predict(train_sample_log)\n",
    "    predicted_valid_log = model.predict(valid_sample_log)\n",
    "    predicted_test_log = model.predict(test_sample_log)\n",
    "    print(\"\\n\")\n",
    "    print(\"Regularization rate: \", i)\n",
    "    print (\"Prediction accuracy in train set : \", \n",
    "           metrics.accuracy_score(train_label_sample_log, predicted_train_log))\n",
    "    print (\"Prediction accuracy in validation set : \", \n",
    "           metrics.accuracy_score(valid_label_sample_log, predicted_valid_log))\n",
    "    print (\"Prediction accuracy in test set : \", \n",
    "           metrics.accuracy_score(test_label_sample_log, predicted_test_log))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, when we adjusting the regularization weight (`C` is the inverse coefficient of regularization. Refer page [Sklearn-Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) test prediction accuracy is changed  so as validation set. This means, the classifier will perform best with right value of `beta` with out losing it's ability to learn the general trend rather than overfitting to training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat into shape desirable for `tensorflow`\n",
    "\n",
    "- Data as a flat matrix\n",
    "- Labels as float 1-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_train = np.random.choice(np.arange(len(train_dataset_log)), 60000, replace=False)\n",
    "idx_valid = np.random.choice(np.arange(len(valid_dataset_log)), 10000, replace=False)\n",
    "idx_test = np.random.choice(np.arange(len(test_dataset_log)), 10000, replace=False)\n",
    "train_sample_log = train_dataset_log[idx_train]\n",
    "train_label_sample_log = train_labels_log[idx_train]\n",
    "valid_sample_log = valid_dataset_log[idx_valid]\n",
    "valid_label_sample_log = valid_labels_log[idx_valid]\n",
    "test_sample_log = test_dataset_log[idx_test]\n",
    "test_label_sample_log = test_labels_log[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (60000, 784) (60000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_sample_log, train_label_sample_log)\n",
    "valid_dataset, valid_labels = reformat(valid_sample_log, valid_label_sample_log)\n",
    "test_dataset, test_labels = reformat(test_sample_log, test_label_sample_log)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with `L2` regression using `tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is to expedite the process \n",
    "train_subset = 10500\n",
    "# This is a good beta value to start with\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # They're all constants.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables    \n",
    "    # They are variables we want to update and optimize.\n",
    "    beta = tf.Variable(0, dtype=tf.float32)\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases \n",
    "    # Original loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels) )\n",
    "    \n",
    "    # Loss function using L2 Regularization\n",
    "    regularizer = tf.nn.l2_loss(weights)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax( tf.matmul(tf_valid_dataset, weights) + biases )\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "---------------------------------------\n",
      "Regularization coefficient (beta):  1\n",
      "Loss at step 0: 3131.05419921875\n",
      "Training accuracy: 13.4\n",
      "Validation accuracy: 19.4\n",
      "Loss at step 100: 1.8850555419921875\n",
      "Training accuracy: 58.0\n",
      "Validation accuracy: 56.7\n",
      "Loss at step 200: 1.9128667116165161\n",
      "Training accuracy: 56.3\n",
      "Validation accuracy: 55.6\n",
      "Loss at step 300: 1.9170057773590088\n",
      "Training accuracy: 55.2\n",
      "Validation accuracy: 56.0\n",
      "Loss at step 400: 1.9099663496017456\n",
      "Training accuracy: 56.5\n",
      "Validation accuracy: 56.2\n",
      "Loss at step 500: 1.9082928895950317\n",
      "Training accuracy: 55.7\n",
      "Validation accuracy: 56.0\n",
      "Loss at step 600: 1.931567668914795\n",
      "Training accuracy: 56.1\n",
      "Validation accuracy: 54.7\n",
      "Loss at step 700: 2.0014266967773438\n",
      "Training accuracy: 54.3\n",
      "Validation accuracy: 53.4\n",
      "Loss at step 800: 1.9024858474731445\n",
      "Training accuracy: 56.2\n",
      "Validation accuracy: 56.9\n",
      "Test accuracy: 62.9\n",
      "---------------------------------------\n",
      "Regularization coefficient (beta):  0.01\n",
      "Loss at step 0: 1.5275335311889648\n",
      "Training accuracy: 57.8\n",
      "Validation accuracy: 70.1\n",
      "Loss at step 100: 0.6723777055740356\n",
      "Training accuracy: 84.2\n",
      "Validation accuracy: 81.5\n",
      "Loss at step 200: 0.6638653874397278\n",
      "Training accuracy: 84.5\n",
      "Validation accuracy: 81.7\n",
      "Loss at step 300: 0.6616016030311584\n",
      "Training accuracy: 84.7\n",
      "Validation accuracy: 81.5\n",
      "Loss at step 400: 0.6607952117919922\n",
      "Training accuracy: 84.8\n",
      "Validation accuracy: 81.5\n",
      "Loss at step 500: 0.6604449152946472\n",
      "Training accuracy: 84.9\n",
      "Validation accuracy: 81.6\n",
      "Loss at step 600: 0.6602657437324524\n",
      "Training accuracy: 84.9\n",
      "Validation accuracy: 81.6\n",
      "Loss at step 700: 0.6601618528366089\n",
      "Training accuracy: 84.9\n",
      "Validation accuracy: 81.5\n",
      "Loss at step 800: 0.6600958704948425\n",
      "Training accuracy: 84.9\n",
      "Validation accuracy: 81.5\n",
      "Test accuracy: 89.4\n",
      "---------------------------------------\n",
      "Regularization coefficient (beta):  0.001\n",
      "Loss at step 0: 0.5851899981498718\n",
      "Training accuracy: 84.9\n",
      "Validation accuracy: 81.5\n",
      "Loss at step 100: 0.5579517483711243\n",
      "Training accuracy: 85.5\n",
      "Validation accuracy: 81.7\n",
      "Loss at step 200: 0.5456955432891846\n",
      "Training accuracy: 85.8\n",
      "Validation accuracy: 81.7\n",
      "Loss at step 300: 0.5374457240104675\n",
      "Training accuracy: 86.1\n",
      "Validation accuracy: 81.8\n",
      "Loss at step 400: 0.5313544273376465\n",
      "Training accuracy: 86.4\n",
      "Validation accuracy: 81.4\n",
      "Loss at step 500: 0.5266546607017517\n",
      "Training accuracy: 86.6\n",
      "Validation accuracy: 81.4\n",
      "Loss at step 600: 0.5229324698448181\n",
      "Training accuracy: 86.8\n",
      "Validation accuracy: 81.4\n",
      "Loss at step 700: 0.5199302434921265\n",
      "Training accuracy: 86.9\n",
      "Validation accuracy: 81.4\n",
      "Loss at step 800: 0.5174781680107117\n",
      "Training accuracy: 87.1\n",
      "Validation accuracy: 81.5\n",
      "Test accuracy: 89.5\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for i in [1, 0.01, 0.001]:\n",
    "        print(\"---------------------------------------\")\n",
    "        print(\"Regularization coefficient (beta): \", i)\n",
    "        for step in range(num_steps):\n",
    "            # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "            # and get the loss value and the training predictions returned as numpy\n",
    "            # arrays.\n",
    "            session.run(beta.assign(i))\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "            if (step % 100 == 0):\n",
    "                print('Loss at step {}: {}'.format(step, l))\n",
    "                print('Training accuracy: {:.1f}'.format(accuracy(predictions, \n",
    "                                                         train_labels[:train_subset, :])))\n",
    "                # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "                # just to get that one numpy array. Note that it recomputes all its graph\n",
    "                # dependencies.\n",
    "            \n",
    "                # You don't have to do .eval above because we already ran the session for the\n",
    "                # train_prediction\n",
    "                print('Validation accuracy: {:.1f}'.format(accuracy(valid_prediction.eval(), \n",
    "                                                           valid_labels)))\n",
    "        print('Test accuracy: {:.1f}'.format(accuracy(test_prediction.eval(), test_labels))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three layer neural network with `relu` and `l2` regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "no_hidden_layer_neurons = 1024\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) #10000 x 784\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) # 10000 x 1\n",
    "    tf_valid_dataset = tf.constant(valid_dataset) \n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.Variable(0, dtype=tf.float32)\n",
    "    \n",
    "    # Hidden layer\n",
    "    w1 = tf.Variable(tf.truncated_normal([image_size * image_size, no_hidden_layer_neurons])) # 784 x 1024\n",
    "    b1 = tf.Variable(tf.zeros([no_hidden_layer_neurons])) # 1024 x1 \n",
    "    h1 = tf.add(tf.matmul(tf_train_dataset, w1), b1)\n",
    "    h1_activation = tf.nn.relu(h1)\n",
    "\n",
    "    \n",
    "    # Output layer\n",
    "    w2 = tf.Variable(tf.truncated_normal([no_hidden_layer_neurons, num_labels])) #1024 x 10\n",
    "    b2 = tf.Variable(tf.random_normal([num_labels])) #10x1\n",
    "    \n",
    "    h2 = tf.add(tf.matmul(h1_activation, w2), b2)\n",
    "    \n",
    "    # Converting to softmax for finding prediction accuracy in training data\n",
    "    train_prediction = tf.nn.softmax(h2)\n",
    "    \n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=h2))\n",
    "    \n",
    "    # Loss function with L2 Regularization with beta=0.01\n",
    "    regularizers = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Prediction\n",
    "    def prediction(data):\n",
    "        h = tf.add(tf.matmul(data, w1), b1)\n",
    "        h_relu = tf.nn.relu(h)\n",
    "        prediction = tf.add(tf.matmul(h_relu, w2), b2)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    valid_prediction = prediction(tf_valid_dataset)\n",
    "    test_prediction = prediction(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session for the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "---------------------------------------\n",
      "Regularization coefficient (beta):  1\n",
      "Minibatch loss at step 0: 314964.843750\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 19.3%\n",
      "Minibatch loss at step 500: 2.304509\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 9.9%\n",
      "Minibatch loss at step 1000: 2.306274\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 1500: 2.305539\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 9.9%\n",
      "Minibatch loss at step 2000: 2.305341\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 9.9%\n",
      "Minibatch loss at step 2500: 2.313149\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 9.9%\n",
      "Minibatch loss at step 3000: 2.304334\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 10.6%\n",
      "Test accuracy: 10.5%\n",
      "---------------------------------------\n",
      "Regularization coefficient (beta):  0.01\n",
      "Minibatch loss at step 0: 2.305953\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 10.6%\n",
      "Minibatch loss at step 500: 0.691224\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1000: 0.585033\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1500: 0.608985\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 0.658855\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2500: 0.640725\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 3000: 0.630335\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.3%\n",
      "---------------------------------------\n",
      "Regularization coefficient (beta):  0.001\n",
      "Minibatch loss at step 0: 0.470662\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 500: 0.408143\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1000: 0.219618\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1500: 0.229645\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 2000: 0.222507\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2500: 0.191927\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 3000: 0.245360\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 83.9%\n",
      "Test accuracy: 91.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for i in [1, 0.01, 0.001]:\n",
    "        print(\"---------------------------------------\")\n",
    "        print(\"Regularization coefficient (beta): \", i)\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            session.run(beta.assign(i))\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the results, with suitable value of `beta` the accuracy poised to 91.0 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of extreme overfitting using very little data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "graph = tf.Graph()\n",
    "no_hidden_layer_neurons = 1024\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size)) #10000 x 784\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) # 10000 x 1\n",
    "    tf_valid_dataset = tf.constant(valid_dataset) \n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    beta = tf.Variable(0, dtype=tf.float32)\n",
    "    \n",
    "    # Hidden layer\n",
    "    w1 = tf.Variable(tf.truncated_normal([image_size * image_size, no_hidden_layer_neurons])) # 784 x 1024\n",
    "    b1 = tf.Variable(tf.zeros([no_hidden_layer_neurons])) # 1024 x1 \n",
    "    h1 = tf.add(tf.matmul(tf_train_dataset, w1), b1)\n",
    "    h1_activation = tf.nn.relu(h1)\n",
    "\n",
    "    \n",
    "    # Output layer\n",
    "    w2 = tf.Variable(tf.truncated_normal([no_hidden_layer_neurons, num_labels])) #1024 x 10\n",
    "    b2 = tf.Variable(tf.random_normal([num_labels])) #10x1\n",
    "    \n",
    "    h2 = tf.add(tf.matmul(h1_activation, w2), b2)\n",
    "    \n",
    "    # Converting to softmax for finding prediction accuracy in training data\n",
    "    train_prediction = tf.nn.softmax(h2)\n",
    "    \n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=h2))\n",
    "    \n",
    "    # Loss function with L2 Regularization with beta=0.01\n",
    "#     regularizers = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2)\n",
    "#     loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Prediction\n",
    "    def prediction(data):\n",
    "        h = tf.add(tf.matmul(data, w1), b1)\n",
    "        h_relu = tf.nn.relu(h)\n",
    "        prediction = tf.add(tf.matmul(h_relu, w2), b2)\n",
    "        return prediction\n",
    "\n",
    "\n",
    "    valid_prediction = prediction(tf_valid_dataset)\n",
    "    test_prediction = prediction(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 298.97283935546875\n",
      "Minibatch accuracy: 12.0\n",
      "Validation accuracy: 21.1\n",
      "Minibatch loss at step 500: 1.4662634839623934e-07\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 75.1\n",
      "Minibatch loss at step 1000: 8.344616020394824e-08\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 75.1\n",
      "Minibatch loss at step 1500: 6.079655179291876e-08\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 75.1\n",
      "Minibatch loss at step 2000: 4.7683602844017514e-08\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 75.1\n",
      "Minibatch loss at step 2500: 4.0531077161176654e-08\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 75.1\n",
      "Minibatch loss at step 3000: 3.576272433747363e-08\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 75.1\n",
      "Test accuracy: 83.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps = 3001\n",
    "\n",
    "train_dataset_2 = train_dataset[:500, :]\n",
    "train_labels_2 = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "#     session.run(beta.assign(0.01))\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Here we can see the extreme case of overfitting which lead to testing data accuracy to mere 83% but training data accuracy is 100%. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout and variation in extreme overfitting case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout should only be introduced during training, not evaluation for stochastic stability.\n",
    "- This model is three layer NN with a `relu` layer\n",
    "- The keep_prob value is used to control the [dropout rate](https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/#dropout) used when training the neural network. Essentially, it means that each connection between layers (in this case between the last densely connected layer and the readout layer) will only be used with probability 0.5 when training. This reduces overfitting. For more information on the theory of dropout, you can see the original paper by [Srivastava et al](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). To see how to use it in TensorFlow, see the documentation on the tf.nn.dropout() operator.\n",
    "\n",
    "- The keep_prob value is fed in via a placeholder so that the same graph can be used for training (with keep_prob = 0.5) and evaluation (with keep_prob = 1.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes= 1024\n",
    "batch_size = 128\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    relu_layer_dropout = tf.nn.dropout(relu_layer, keep_prob)\n",
    "    \n",
    "    logits_2 = tf.matmul(relu_layer_dropout, weights_2) + biases_2\n",
    "    # Normal loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_2, labels = tf_train_labels))\n",
    "    # Loss function with L2 Regularization with beta=0.01\n",
    "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2)\n",
    "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for validation \n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for test\n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3652.39013671875\n",
      "Minibatch accuracy: 8.6\n",
      "Validation accuracy: 29.4\n",
      "Minibatch loss at step 500: 21.442420959472656\n",
      "Minibatch accuracy: 88.3\n",
      "Validation accuracy: 83.8\n",
      "Minibatch loss at step 1000: 1.0529266595840454\n",
      "Minibatch accuracy: 82.8\n",
      "Validation accuracy: 82.7\n",
      "Minibatch loss at step 1500: 0.7329974174499512\n",
      "Minibatch accuracy: 84.4\n",
      "Validation accuracy: 83.0\n",
      "Minibatch loss at step 2000: 0.8863440155982971\n",
      "Minibatch accuracy: 81.2\n",
      "Validation accuracy: 82.3\n",
      "Minibatch loss at step 2500: 0.7034103274345398\n",
      "Minibatch accuracy: 85.2\n",
      "Validation accuracy: 83.5\n",
      "Minibatch loss at step 3000: 0.7037007808685303\n",
      "Minibatch accuracy: 83.6\n",
      "Validation accuracy: 82.8\n",
      "Test accuracy: 90.8\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In extreme overfitting case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3583.098388671875\n",
      "Minibatch accuracy: 7.0\n",
      "Validation accuracy: 38.9\n",
      "Minibatch loss at step 500: 21.135400772094727\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 78.4\n",
      "Minibatch loss at step 1000: 0.4926719069480896\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 79.9\n",
      "Minibatch loss at step 1500: 0.3133500814437866\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 80.3\n",
      "Minibatch loss at step 2000: 0.3020404875278473\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 80.4\n",
      "Minibatch loss at step 2500: 0.2865172326564789\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 80.0\n",
      "Minibatch loss at step 3000: 0.28297892212867737\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 79.9\n",
      "Test accuracy: 87.6\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "train_dataset_2 = train_dataset[:500, :]\n",
    "train_labels_2 = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels_2.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset_2[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels_2[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can see the same `overfitting` problem here also. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a better and complex (`deep`) models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4 layer MLPs with 1024-300-50 hidden neurons respectively\n",
    "- Exponential decay of the learning rate\n",
    "- L2 Regularization\n",
    "- Drop out layers\n",
    "- RELU layers\n",
    "- tanh Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Graph - 1 using `relu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.0001\n",
    "\n",
    "hidden_nodes_1 = 1024\n",
    "hidden_nodes_2 = 300\n",
    "hidden_nodes_3 = 50\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    '''Input Data'''\n",
    "    # For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    '''Variables'''\n",
    "    # Hidden RELU layer 1\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_1], \n",
    "                                                stddev=math.sqrt(2.0/(image_size*image_size))))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
    "\n",
    "    # Hidden RELU layer 2\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2], \n",
    "                                                stddev=math.sqrt(2.0/hidden_nodes_1)))\n",
    "    biases_2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "    \n",
    "    # Hidden RELU layer 3\n",
    "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_2, hidden_nodes_3], \n",
    "                                                stddev=math.sqrt(2.0/hidden_nodes_2)))\n",
    "    biases_3 = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
    "    \n",
    "    # Output layer\n",
    "    weights_out = tf.Variable(tf.truncated_normal([hidden_nodes_3, num_labels], \n",
    "                                                stddev=math.sqrt(2.0/hidden_nodes_3)))\n",
    "    biases_out = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    '''Training computation'''\n",
    "    \n",
    "    # Hidden RELU layer 1\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    hidden_layer_1 = tf.nn.relu(logits_1)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "#     keep_prob = tf.placeholder(tf.float32)\n",
    "#     hidden_layer_1_dropout = tf.nn.dropout(hidden_layer_1, keep_prob)\n",
    "    \n",
    "    \n",
    "    # Hidden RELU layer 2\n",
    "    logits_2 = tf.matmul(hidden_layer_1, weights_2) + biases_2\n",
    "    hidden_layer_2 = tf.nn.relu(logits_2)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "#     hidden_layer_2_dropout = tf.nn.dropout(hidden_layer_2, keep_prob)\n",
    "    \n",
    "    # Hidden RELU layer 3\n",
    "    logits_3 = tf.matmul(hidden_layer_2, weights_3) + biases_3\n",
    "    hidden_layer_3 = tf.nn.relu(logits_3)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "#     hidden_layer_3_dropout = tf.nn.dropout(hidden_layer_3, keep_prob)\n",
    "    \n",
    "    # Output layer\n",
    "    logits_4 = tf.matmul(hidden_layer_3, weights_out) + biases_out\n",
    "    \n",
    "    # Normal loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_4, labels = tf_train_labels))\n",
    "    # Loss function with L2 Regularization with decaying learning rate beta=0.5\n",
    "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + \\\n",
    "                   tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_out)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    '''Optimizer'''\n",
    "    # Decaying learning rate\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    start_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_4)\n",
    "    \n",
    "    # Predictions for validation \n",
    "    valid_logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    valid_relu_1 = tf.nn.relu(valid_logits_1)\n",
    "    \n",
    "    valid_logits_2 = tf.matmul(valid_relu_1, weights_2) + biases_2\n",
    "    valid_relu_2 = tf.nn.relu(valid_logits_2)\n",
    "    \n",
    "    valid_logits_3 = tf.matmul(valid_relu_2, weights_3) + biases_3\n",
    "    valid_relu_3 = tf.nn.relu(valid_logits_3)\n",
    "    \n",
    "    valid_logits_4 = tf.matmul(valid_relu_3, weights_out) + biases_out    \n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(valid_logits_4)\n",
    "    \n",
    "    # Predictions for test\n",
    "    test_logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    test_relu_1 = tf.nn.relu(test_logits_1)\n",
    "    \n",
    "    test_logits_2 = tf.matmul(test_relu_1, weights_2) + biases_2\n",
    "    test_relu_2 = tf.nn.relu(test_logits_2)\n",
    "    \n",
    "    test_logits_3 = tf.matmul(test_relu_2, weights_3) + biases_3\n",
    "    test_relu_3 = tf.nn.relu(test_logits_3)\n",
    "    \n",
    "    test_logits_4 = tf.matmul(test_relu_3, weights_out) + biases_out\n",
    "\n",
    "    test_prediction = tf.nn.softmax(test_logits_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph - 2 using `tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "beta = 0.0005\n",
    "\n",
    "hidden_nodes_1 = 1024\n",
    "hidden_nodes_2 = 300\n",
    "hidden_nodes_3 = 50\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    '''Input Data'''\n",
    "    # For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    '''Variables'''\n",
    "    # Hidden RELU layer 1\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_1], \n",
    "                                                stddev=math.sqrt(2.0/(image_size*image_size))))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
    "\n",
    "    # Hidden RELU layer 2\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes_1, hidden_nodes_2], \n",
    "                                                stddev=math.sqrt(2.0/hidden_nodes_1)))\n",
    "    biases_2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "    \n",
    "    # Hidden RELU layer 3\n",
    "    weights_3 = tf.Variable(tf.truncated_normal([hidden_nodes_2, hidden_nodes_3], \n",
    "                                                stddev=math.sqrt(2.0/hidden_nodes_2)))\n",
    "    biases_3 = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
    "    \n",
    "    # Output layer\n",
    "    weights_out = tf.Variable(tf.truncated_normal([hidden_nodes_3, num_labels], \n",
    "                                                stddev=math.sqrt(2.0/hidden_nodes_3)))\n",
    "    biases_out = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    '''Training computation'''\n",
    "    \n",
    "    # Hidden RELU layer 1\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    hidden_layer_1 = tf.tanh(logits_1)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "#     keep_prob = tf.placeholder(tf.float32)\n",
    "#     hidden_layer_1_dropout = tf.nn.dropout(hidden_layer_1, keep_prob)\n",
    "    \n",
    "    \n",
    "    # Hidden RELU layer 2\n",
    "    logits_2 = tf.matmul(hidden_layer_1, weights_2) + biases_2\n",
    "    hidden_layer_2 = tf.tanh(logits_2)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "#     hidden_layer_2_dropout = tf.nn.dropout(hidden_layer_2, keep_prob)\n",
    "    \n",
    "    # Hidden RELU layer 3\n",
    "    logits_3 = tf.matmul(hidden_layer_2, weights_3) + biases_3\n",
    "    hidden_layer_3 = tf.tanh(logits_3)\n",
    "    # Dropout on hidden layer: RELU layer\n",
    "#     hidden_layer_3_dropout = tf.nn.dropout(hidden_layer_3, keep_prob)\n",
    "    \n",
    "    # Output layer\n",
    "    logits_4 = tf.matmul(hidden_layer_3, weights_out) + biases_out\n",
    "    \n",
    "    # Normal loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits_4, labels = tf_train_labels))\n",
    "    # Loss function with L2 Regularization with decaying learning rate beta=0.5\n",
    "    regularizers = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + \\\n",
    "                   tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_out)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "    '''Optimizer'''\n",
    "    # Decaying learning rate\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    start_learning_rate = 0.1\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_4)\n",
    "    \n",
    "    # Predictions for validation \n",
    "    valid_logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    valid_relu_1 = tf.tanh(valid_logits_1)\n",
    "    \n",
    "    valid_logits_2 = tf.matmul(valid_relu_1, weights_2) + biases_2\n",
    "    valid_relu_2 = tf.tanh(valid_logits_2)\n",
    "    \n",
    "    valid_logits_3 = tf.matmul(valid_relu_2, weights_3) + biases_3\n",
    "    valid_relu_3 = tf.tanh(valid_logits_3)\n",
    "    \n",
    "    valid_logits_4 = tf.matmul(valid_relu_3, weights_out) + biases_out    \n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(valid_logits_4)\n",
    "    \n",
    "    # Predictions for test\n",
    "    test_logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    test_relu_1 = tf.tanh(test_logits_1)\n",
    "    \n",
    "    test_logits_2 = tf.matmul(test_relu_1, weights_2) + biases_2\n",
    "    test_relu_2 = tf.tanh(test_logits_2)\n",
    "    \n",
    "    test_logits_3 = tf.matmul(test_relu_2, weights_3) + biases_3\n",
    "    test_relu_3 = tf.tanh(test_logits_3)\n",
    "    \n",
    "    test_logits_4 = tf.matmul(test_relu_3, weights_out) + biases_out\n",
    "\n",
    "    test_prediction = tf.nn.softmax(test_logits_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session - For graph 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.4557242393493652\n",
      "Minibatch accuracy: 10.9\n",
      "Validation accuracy: 26.8\n",
      "Minibatch loss at step 1000: 0.5649263262748718\n",
      "Minibatch accuracy: 86.7\n",
      "Validation accuracy: 85.7\n",
      "Minibatch loss at step 2000: 0.3081977367401123\n",
      "Minibatch accuracy: 96.9\n",
      "Validation accuracy: 86.4\n",
      "Minibatch loss at step 3000: 0.46760421991348267\n",
      "Minibatch accuracy: 90.6\n",
      "Validation accuracy: 87.4\n",
      "Minibatch loss at step 4000: 0.2653837502002716\n",
      "Minibatch accuracy: 96.1\n",
      "Validation accuracy: 87.2\n",
      "Minibatch loss at step 5000: 0.23846019804477692\n",
      "Minibatch accuracy: 96.9\n",
      "Validation accuracy: 87.6\n",
      "Minibatch loss at step 6000: 0.17572139203548431\n",
      "Minibatch accuracy: 98.4\n",
      "Validation accuracy: 87.4\n",
      "Minibatch loss at step 7000: 0.139068141579628\n",
      "Minibatch accuracy: 99.2\n",
      "Validation accuracy: 87.5\n",
      "Minibatch loss at step 8000: 0.12944065034389496\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 88.0\n",
      "Minibatch loss at step 9000: 0.15894877910614014\n",
      "Minibatch accuracy: 98.4\n",
      "Validation accuracy: 87.8\n",
      "Minibatch loss at step 10000: 0.12136251479387283\n",
      "Minibatch accuracy: 100.0\n",
      "Validation accuracy: 88.4\n",
      "Minibatch loss at step 11000: 0.13212862610816956\n",
      "Minibatch accuracy: 99.2\n",
      "Validation accuracy: 88.3\n",
      "Minibatch loss at step 12000: 0.1424584835767746\n",
      "Minibatch accuracy: 99.2\n",
      "Validation accuracy: 87.5\n",
      "Minibatch loss at step 13000: 0.13000421226024628\n",
      "Minibatch accuracy: 99.2\n",
      "Validation accuracy: 88.2\n",
      "Minibatch loss at step 14000: 0.15219008922576904\n",
      "Minibatch accuracy: 99.2\n",
      "Validation accuracy: 88.0\n",
      "Test accuracy: 94.7\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} #keep_prob : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Finally we have the highest accuracy so far i.e. 94.7 % and it can be further improved using more data and deeper models` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
